# 数据构造过程

【腾讯文档】数据准备流程图
https://docs.qq.com/flowchart/DUlRUY2tWelN4Y0xj



![image-20221014153536189](https://yddimagebed.oss-cn-hangzhou.aliyuncs.com/img/image-20221014153536189.png)

### step1 数据的收集

通过爬虫或者其他手段收集文本信息。

收集到的信息多样性较高，通过一些手段将文本保存为txt文本

### step2数据的分类

针对不同来源的数据，根据来源可信度进行分类

例如：

1. 新闻联播新闻稿（可靠性：90）
2. 学习强国官网（可靠性：80）
3. 新华社（可靠性：80）
4. 官媒微博（可靠性：60）
5. 民间媒体（可靠性：50）

对于可靠性较高的文本，可以直接作为母本数据，并且提高其在训练数据中的权重。

对于可靠性较差的文本，需要先通过数据清洗，才能作为母本数据，并且权重降低

### step3数据的清洗

dataClean.py

对于可靠性不高的文本，其本身就包含错误，所以需要通过多种方式进行清洗。

1.规则的方式清洗

2.深度模型判错清洗

​最好是用多个模型进行联合判错，宁可错杀，不可放过。

3.迭代式数据清洗（需完善）

我们发现，由于单语语料采集自互联网，本身就含有比例不低的各类文本错误，可能对训练模型的结果产生影响，因此我们使用了一种非常简单的迭代式数据清洗方案。具体的做法是：

1. 对于单语语料X，我们先加噪生成伪数据Y，利用平行数据(X, Y)训练得到模型S；
2. 然后利用模型S对于X进行推理，得到纠正后结果X‘，此时仍然对X加噪得到伪数据Y’，利用平行数据(X‘, Y’)训练新模型S‘；
3. 反复迭代上述过程，直到获得最终模型S_final。

上述做法的好处有两点，一个是能不断缓解原始单语数据中潜在噪音的影响，另一个就是相当于在做自知识蒸馏；实践中我们发现上述方法（3次迭代）能够提升模型性能2-3个点。

### step4构造数据集

wrongMaker.py

所有的母本数据构造完成之后，母本数据就是全是正确的文本，我们可以人为的造出错误，构造训练数据集。

通过在对应母本数据的文件名最后加上'_num'，来表示权重。1代表取一次，10代表取10次。不用担心会重复，每次都会独立判断是否造错。

目前设置错误比例的一些情况：

baseline的比例中不造错的比例要高一些，因为主要文本错误并不是很多。然后交换位子的情况尽可能少一些，0.1有点多。

